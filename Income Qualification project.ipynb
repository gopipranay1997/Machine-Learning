{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Understand the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train=pd.read_csv('train3.csv')\n",
    "df_income_test=pd.read_csv('test3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the columns for different datatypes:\n",
    "print('Integer Type:')\n",
    "print(df_income_train.select_dtypes(np.int64).columns)\n",
    "print('\\n')\n",
    "print('Float Type:')\n",
    "print(df_income_train.select_dtypes(np.float64).columns)\n",
    "print('\\n')\n",
    "print('Object Type:')\n",
    "print(df_income_train.select_dtypes(np.object).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Checking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.select_dtypes('int64').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with null values\n",
    "null_counts=df_income_train.select_dtypes('int64').isnull().sum()\n",
    "null_counts[null_counts>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.select_dtypes('float64').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with null values\n",
    "null_counts=df_income_train.select_dtypes('float64').isnull().sum()\n",
    "null_counts[null_counts>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.select_dtypes('object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns with null values\n",
    "null_counts=df_income_train.select_dtypes('object').isnull().sum()\n",
    "null_counts[null_counts>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We also noticed that object type features dependency, edjefe, edjefa have mixed values.\n",
    "Lets fix the data for features with null values and features with mixed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 Lets fix the column with mixed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation for these columns:\n",
    "dependency: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n",
    "edjefe: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n",
    "edjefa: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n",
    "\n",
    "For these three variables, it seems \"yes\" = 1 and \"no\" = 0. We can correct the variables using a mapping and convert to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping={'yes':1,'no':0}\n",
    "\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['dependency'] =df['dependency'].replace(mapping).astype(np.float64)\n",
    "    df['edjefe'] =df['edjefe'].replace(mapping).astype(np.float64)\n",
    "    df['edjefa'] =df['edjefa'].replace(mapping).astype(np.float64)\n",
    "    \n",
    "df_income_train[['dependency','edjefe','edjefa']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Lets fix the column with null values   \n",
    "\n",
    "According to the documentation for these columns:   \n",
    "\n",
    "   v2a1       (total nulls: 6860) : Monthly rent payment   \n",
    "   v18q1      (total nulls: 7342) : number of tablets household owns   \n",
    "   rez_esc    (total nulls: 7928) : Years behind in school   \n",
    "   meaneduc   (total nulls: 5) : average years of education for adults (18+)   \n",
    "   SQBmeaned  (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Lets look at v2a1 (total nulls: 6860) : Monthly rent payment \n",
    "# why the null values, Lets look at few rows with nulls in v2a1\n",
    "# Columns related to  Monthly rent payment\n",
    "# tipovivi1, =1 own and fully paid house\n",
    "# tipovivi2, \"=1 own,  paying in installments\"\n",
    "# tipovivi3, =1 rented\n",
    "# tipovivi4, =1 precarious \n",
    "# tipovivi5, \"=1 other(assigned,  borrowed)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_income_train[df_income_train['v2a1'].isnull()].head()\n",
    "\n",
    "columns=['tipovivi1','tipovivi2','tipovivi3','tipovivi4','tipovivi5']\n",
    "data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables indicating home ownership\n",
    "own_variables = [x for x in df_income_train if x.startswith('tipo')]\n",
    "\n",
    "# Plot of the home ownership variables for home missing rent payments\n",
    "df_income_train.loc[df_income_train['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n",
    "                                                                        color = 'green',\n",
    "                                                              edgecolor = 'k', linewidth = 2);\n",
    "plt.xticks([0, 1, 2, 3, 4],\n",
    "           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n",
    "          rotation = 20)\n",
    "plt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the above data it makes sense that when the house is fully paid, there will be no monthly rent payment.\n",
    "#Lets add 0 for all the null values.\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['v2a1'].fillna(value=0, inplace=True)\n",
    "\n",
    "df_income_train[['v2a1']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lets look at v18q1 (total nulls: 7342) : number of tablets household owns \n",
    "# why the null values, Lets look at few rows with nulls in v18q1\n",
    "# Columns related to  number of tablets household owns \n",
    "# v18q, owns a tablet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since this is a household variable, it only makes sense to look at it on a household level, \n",
    "# so we'll only select the rows for the head of household.\n",
    "\n",
    "# Heads of household\n",
    "heads = df_income_train.loc[df_income_train['parentesco1'] == 1].copy()\n",
    "heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "col='v18q1'\n",
    "df_income_train[col].value_counts().sort_index().plot.bar(color = 'blue',\n",
    "                                             edgecolor = 'k',\n",
    "                                             linewidth = 2)\n",
    "plt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at the above data it makes sense that when owns a tablet column is 0, there will be no number of tablets household owns.\n",
    "#Lets add 0 for all the null values.\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['v18q1'].fillna(value=0, inplace=True)\n",
    "\n",
    "df_income_train[['v18q1']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Lets look at rez_esc    (total nulls: 7928) : Years behind in school  \n",
    "# why the null values, Lets look at few rows with nulls in rez_esc\n",
    "# Columns related to Years behind in school \n",
    "# Age in years\n",
    "\n",
    "# Lets look at the data with not null values first.\n",
    "df_income_train[df_income_train['rez_esc'].notnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From the above , we see that when min age is 7 and max age is 17 for Years, then the 'behind in school' column has a value.\n",
    "#Lets confirm\n",
    "df_income_train.loc[df_income_train['rez_esc'].isnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train.loc[(df_income_train['rez_esc'].isnull() & ((df_income_train['age'] > 7) & (df_income_train['age'] < 17)))]['age'].describe()\n",
    "#There is one value that has Null for the 'behind in school' column with age between 7 and 17 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_income_train[(df_income_train['age'] ==10) & df_income_train['rez_esc'].isnull()].head()\n",
    "df_income_train[(df_income_train['Id'] =='ID_f012e4242')].head()\n",
    "#there is only one member in household for the member with age 10 and who is 'behind in school'. This explains why the member is \n",
    "#behind in school."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from above we see that  the 'behind in school' column has null values \n",
    "# Lets use the above to fix the data\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['rez_esc'].fillna(value=0, inplace=True)\n",
    "df_income_train[['rez_esc']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at meaneduc   (total nulls: 5) : average years of education for adults (18+)  \n",
    "# why the null values, Lets look at few rows with nulls in meaneduc\n",
    "# Columns related to average years of education for adults (18+)  \n",
    "# edjefe, years of education of male head of household, based on the interaction of escolari (years of education),\n",
    "#    head of household and gender, yes=1 and no=0\n",
    "# edjefa, years of education of female head of household, based on the interaction of escolari (years of education), \n",
    "#    head of household and gender, yes=1 and no=0 \n",
    "# instlevel1, =1 no level of education\n",
    "# instlevel2, =1 incomplete primary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_income_train[df_income_train['meaneduc'].isnull()].head()\n",
    "\n",
    "columns=['edjefe','edjefa','instlevel1','instlevel2']\n",
    "data[columns][data[columns]['instlevel1']>0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above, we find that meaneduc is null when no level of education is 0\n",
    "#Lets fix the data\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['meaneduc'].fillna(value=0, inplace=True)\n",
    "df_income_train[['meaneduc']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at SQBmeaned  (total nulls: 5) : square of the mean years of education of adults (>=18) in the household 142  \n",
    "# why the null values, Lets look at few rows with nulls in SQBmeaned\n",
    "# Columns related to average years of education for adults (18+)  \n",
    "# edjefe, years of education of male head of household, based on the interaction of escolari (years of education),\n",
    "#    head of household and gender, yes=1 and no=0\n",
    "# edjefa, years of education of female head of household, based on the interaction of escolari (years of education), \n",
    "#    head of household and gender, yes=1 and no=0 \n",
    "# instlevel1, =1 no level of education\n",
    "# instlevel2, =1 incomplete primary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_income_train[df_income_train['SQBmeaned'].isnull()].head()\n",
    "\n",
    "columns=['edjefe','edjefa','instlevel1','instlevel2']\n",
    "data[columns][data[columns]['instlevel1']>0].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above, we find that SQBmeaned is null when no level of education is 0\n",
    "#Lets fix the data\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df['SQBmeaned'].fillna(value=0, inplace=True)\n",
    "df_income_train[['SQBmeaned']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the overall data\n",
    "null_counts = df_income_train.isnull().sum()\n",
    "null_counts[null_counts > 0].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Lets look at the target column\n",
    "\n",
    "Lets see if records belonging to same household has same target/score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check one household\n",
    "df_income_train[df_income_train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets use Target value of the parent record (head of the household) and update rest. But before that lets check\n",
    "# if all families has a head. \n",
    "\n",
    "households_head = df_income_train.groupby('idhogar')['parentesco1'].sum()\n",
    "\n",
    "# Find households without a head\n",
    "households_no_head = df_income_train.loc[df_income_train['idhogar'].isin(households_head[households_head == 0].index), :]\n",
    "\n",
    "print('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find households without a head and where Target value are different\n",
    "households_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "print('{} Households with no head have different Target value.'.format(sum(households_no_head_equal == False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets fix the data\n",
    "#Set poverty level of the members and the head of the house within a family.\n",
    "# Iterate through each household\n",
    "for household in not_equal.index:\n",
    "    # Find the correct label (for the head of household)\n",
    "    true_target = int(df_income_train[(df_income_train['idhogar'] == household) & (df_income_train['parentesco1'] == 1.0)]['Target'])\n",
    "    \n",
    "    # Set the correct label for all members in the household\n",
    "    df_income_train.loc[df_income_train['idhogar'] == household, 'Target'] = true_target\n",
    "    \n",
    "    \n",
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = df_income_train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Lets check for any bias in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets look at the dataset and plot head of household and Target\n",
    "# 1 = extreme poverty 2 = moderate poverty 3 = vulnerable households 4 = non vulnerable households \n",
    "target_counts = heads['Target'].value_counts().sort_index()\n",
    "target_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts.plot.bar(figsize = (8, 6),linewidth = 2,edgecolor = 'k',title=\"Target vs Total_Count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme poverty is the smallest count in the train dataset. The dataset is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Lets look at the Squared Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‘SQBescolari’\n",
    "‘SQBage’\n",
    "‘SQBhogar_total’\n",
    "‘SQBedjefe’\n",
    "‘SQBhogar_nin’\n",
    "‘SQBovercrowding’\n",
    "‘SQBdependency’\n",
    "‘SQBmeaned’\n",
    "‘agesq’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets remove them\n",
    "\n",
    "print(df_income_train.shape)\n",
    "\n",
    "cols=['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe','SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']\n",
    "\n",
    "\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = cols,inplace=True)\n",
    "\n",
    "\n",
    "print(df_income_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ = ['Id', 'idhogar', 'Target']\n",
    "\n",
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone']\n",
    "\n",
    "ind_ordered = ['rez_esc', 'escolari', 'age']\n",
    "\n",
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for redundant household variables\n",
    "heads = df_income_train.loc[df_income_train['parentesco1'] == 1, :]\n",
    "heads = heads[id_ + hh_bool + hh_cont + hh_ordered]\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = heads.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n",
    "            annot=True, cmap = plt.cm.Accent_r, fmt='.3f');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are several variables here having to do with the size of the house:\n",
    "# r4t3, Total persons in the household\n",
    "# tamhog, size of the household\n",
    "# tamviv, number of persons living in the household\n",
    "# hhsize, household size\n",
    "# hogar_total, # of total individuals in the household\n",
    "# These variables are all highly correlated with one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['tamhog', 'hogar_total', 'r4t3']\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = cols,inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for redundant Individual variables\n",
    "ind = df_income_train[id_ + ind_bool + ind_ordered]\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = ind.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is simply the opposite of male! We can remove the male flag.\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = 'male',inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check area1 and area2 also\n",
    "# area1, =1 zona urbana \n",
    "# area2, =2 zona rural \n",
    "#area2 redundant because we have a column indicating if the house is in a urban zone\n",
    "\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = 'area2',inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally lets delete 'Id', 'idhogar'\n",
    "cols=['Id','idhogar']\n",
    "for df in [df_income_train, df_income_test]:\n",
    "    df.drop(columns = cols,inplace=True)\n",
    "\n",
    "df_income_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 4: Predict the accuracy using random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features=df_income_train.iloc[:,0:-1]\n",
    "y_features=df_income_train.iloc[:,-1]\n",
    "print(x_features.shape)\n",
    "print(y_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,f1_score,classification_report\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_features,y_features,test_size=0.2,random_state=1)\n",
    "rmclassifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmclassifier.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = rmclassifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test,y_predict))\n",
    "print(confusion_matrix(y_test,y_predict))\n",
    "print(classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_testdata = rmclassifier.predict(df_income_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Check the accuracy using random forest with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.1 Checking the score using default 10 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=7\n",
    "kfold=KFold(n_splits=5,random_state=seed,shuffle=True)\n",
    "\n",
    "rmclassifier=RandomForestClassifier(random_state=10,n_jobs = -1)\n",
    "print(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\n",
    "results=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\n",
    "print(results.mean()*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Checking the score using 100 trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees= 100\n",
    "\n",
    "rmclassifier=RandomForestClassifier(n_estimators=100, random_state=10,n_jobs = -1)\n",
    "print(cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy'))\n",
    "results=cross_val_score(rmclassifier,x_features,y_features,cv=kfold,scoring='accuracy')\n",
    "print(results.mean()*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmclassifier.fit(x_train,y_train)\n",
    "y_predict_testdata = rmclassifier.predict(df_income_test)\n",
    "y_predict_testdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the accuracy score, RandomForestClassifier with cross validation has the highest accuracy score of 94.60%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better sense of what is going on inside the RandomForestClassifier model, lets visualize how our model uses the different features and which features have greater effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmclassifier.fit(x_features,y_features)\n",
    "labels = list(x_features)\n",
    "feature_importances = pd.DataFrame({'feature': labels, 'importance': rmclassifier.feature_importances_})\n",
    "feature_importances=feature_importances[feature_importances.importance>0.015]\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.sort_values(by=['importance'], ascending=True, inplace=True)\n",
    "feature_importances['positive'] = feature_importances['importance'] > 0\n",
    "feature_importances.set_index('feature',inplace=True)\n",
    "feature_importances.head()\n",
    "\n",
    "feature_importances.importance.plot(kind='barh', figsize=(11, 6),color = feature_importances.positive.map({True: 'blue', False: 'red'}))\n",
    "plt.xlabel('Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above figure, meaneduc,dependency,overcrowding has significant influence on the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
